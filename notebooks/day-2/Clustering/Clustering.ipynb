{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>MLHEP 2019</center></h1>\n",
    "<h2><center>Seminar: Unsupervised Learning</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "The goal of this seminar is to consider main domains of unsupervised learning and demonstrate algorithms implemented in [scikit-learn](https://scikit-learn.org) library.\n",
    "\n",
    "Topics:\n",
    "- Clustering\n",
    "- Data Scaling\n",
    "- Principal Component Analysis (PCA) (Optionally)\n",
    "- Anomalies Detection (Optionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(centers=3, n_samples=n_samples, random_state=random_state, center_box=(-10, 10))\n",
    "\n",
    "# To play with\n",
    "# X, y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "# X, y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "# X = np.random.rand(n_samples, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y):\n",
    "\n",
    "    # Create an figure with a custom size\n",
    "    # plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    if y is not None:\n",
    "        for cluster_label in np.unique(y):\n",
    "            # Plot all objects with y == i (class 0)\n",
    "            plt.scatter(X[y == cluster_label, 0],     # selects all objects with y == i and the 1st column of X\n",
    "                        X[y == cluster_label, 1],     # selects all objects with y == i and the 2nd column of X\n",
    "                        label=str(cluster_label))     # label for the plot legend\n",
    "    else:\n",
    "        plt.scatter(X[:, 0], X[:, 1], label='samples')\n",
    "\n",
    "    plt.xlabel('X1', size=12) # set up X-axis label\n",
    "    plt.ylabel('X2', size=12) # set up Y-axis label\n",
    "    plt.xticks(size=12)\n",
    "    plt.yticks(size=12)\n",
    "\n",
    "    plt.legend(loc='best', fontsize=12) # create the plot legend and set up it position\n",
    "    plt.grid(b=1) # create grid on the plot\n",
    "\n",
    "    plt.show() # display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X, y=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have $N$ samples and $K$ clusters. Each cluster is described by its center (centroid) with coordinates $\\mu_{j}$. The centroids are estimated by minimizing **within-cluster distance criterion**:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{N} \\min_{\\mu_{k}} \\rho(x_{i}, \\mu_{k}) \\to \\min_{\\mu_{1}, ..., \\mu_{K}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rho(x_{i}, \\mu_{k}) = || x_{i} - \\mu_{k} ||^{2}\n",
    "$$\n",
    "\n",
    "where $x_{i}$ is a sample coordinates, $\\rho(x_{i}, \\mu_{k})$ is distance between the $i$-th sample and the $k$-th cluster's centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means algorithm:**\n",
    "\n",
    "<center><img src=\"img/kmeans-alg.png\" width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/kmean1.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "class MyKmeans(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_clusters=2, max_iter=10, n_init=10):\n",
    "        \"\"\"\n",
    "        K-Means clustering algorithms implementation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_clusters: int\n",
    "            Number of clusters.\n",
    "        max_iters: int\n",
    "            Number of iterations of the centroids search.\n",
    "        n_init: int\n",
    "            Number of different initializations of the centroids.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init = n_init\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _predict_for_centers(self, cluster_centers, X):\n",
    "        \"\"\"\n",
    "        Predict cluster labels based on their centroids.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cluster_centers: numpy.array\n",
    "            Array of the cluster centers.\n",
    "        X: numpy.array\n",
    "            Samples coordinates.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        labels: numpy.array\n",
    "            Predicted cluster labels. Example: labels = [0, 0, 1, 1, 0, 2, ...].\n",
    "        \"\"\"\n",
    "        \n",
    "        object_distances2 = []\n",
    "        for one_cluster_center in cluster_centers:\n",
    "            dist2 = ((X - one_cluster_center)**2).sum(axis=1)\n",
    "            object_distances2.append(dist2)\n",
    "        object_distances2 = np.array(object_distances2)\n",
    "        labels = np.argmin(object_distances2, axis=0)\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _calculate_cluster_centers(self, X, y):\n",
    "        \"\"\"\n",
    "        Estimate cluster centers based on samples in these clusters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: numpy.array\n",
    "            Samples coordinates.\n",
    "        y: numpy.array\n",
    "            Cluster labels of the samples.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        cluster_centers: numpy.array\n",
    "            Estimated cluster centers.\n",
    "        \"\"\"\n",
    "        \n",
    "        cluster_centers = []\n",
    "        cluster_labels = np.unique(y)\n",
    "        \n",
    "        for one_cluster_label in cluster_labels:\n",
    "            one_cluster_center = X[y == one_cluster_label].mean(axis=0)\n",
    "            cluster_centers.append(one_cluster_center)\n",
    "            \n",
    "        return np.array(cluster_centers)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _calculate_cluster_metric(self, cluster_centers, X):\n",
    "        \"\"\"\n",
    "        Calculate within-cluster distance criterion.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cluster_centers: numpy.array\n",
    "            Array of the cluster centers.\n",
    "        X: numpy.array\n",
    "            Samples coordinates.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        criterion: float\n",
    "            The criterion value.\n",
    "        \"\"\"\n",
    "        \n",
    "        object_distances2 = []\n",
    "        for one_cluster_center in cluster_centers:\n",
    "            dist2 = ((X - one_cluster_center)**2).sum(axis=1)\n",
    "            object_distances2.append(dist2)\n",
    "        object_distances2 = np.array(object_distances2)\n",
    "        min_dists2 = np.min(object_distances2, axis=0)\n",
    "        criterion = min_dists2.mean()\n",
    "        \n",
    "        return criterion\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _fit_one_init(self, X):\n",
    "        \"\"\"\n",
    "        Run k-Means algorithm for randomly init cluster centers.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: numpy.array\n",
    "            Samples coordinates.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        cluster_centers: numpy.array\n",
    "            Estimated cluster centers.\n",
    "        metric: float\n",
    "            Within-cluster distance criterion criterion value.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Init cluster centers\n",
    "        cluster_centers = resample(X, n_samples=self.n_clusters, random_state=None, replace=False)\n",
    "        \n",
    "        # Search for cluster centers\n",
    "        for i in range(self.max_iter):\n",
    "            labels = self._predict_for_centers(cluster_centers, X)\n",
    "            cluster_centers = self._calculate_cluster_centers(X, labels)\n",
    "            \n",
    "        # Calculate within-cluster distance criterion\n",
    "        metric = self._calculate_cluster_metric(cluster_centers, X)\n",
    "            \n",
    "        return cluster_centers, metric\n",
    "    \n",
    "        \n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Run k-Means algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: numpy.array\n",
    "            Samples coordinates.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.best_cluster_centers = None\n",
    "        self.best_metric = np.inf\n",
    "        \n",
    "        for i in range(self.n_init):\n",
    "            \n",
    "            # Run K-Means algorithms for randomly init cluster centers\n",
    "            cluster_centers, metric = self._fit_one_init(X)\n",
    "            \n",
    "            # Save the best clusters\n",
    "            if metric < self.best_metric:\n",
    "                self.best_metric = metric\n",
    "                self.best_cluster_centers = cluster_centers\n",
    "                \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: numpy.array\n",
    "            Samples coordinates.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        y: numpy.array\n",
    "            Predicted cluster labels. Example: labels = [0, 0, 1, 1, 0, 2, ...].\n",
    "        \"\"\"\n",
    "        \n",
    "        y = self._predict_for_centers(self.best_cluster_centers, X)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = MyKmeans(n_clusters=3, max_iter=20, n_init=10)\n",
    "clusterer.fit(X)\n",
    "y_pred = clusterer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "**Silhouette Score:**\n",
    "\n",
    "$$\n",
    "s = \\frac{b - a}{max(a, b)}\n",
    "$$\n",
    "\n",
    "- **a**: The mean distance between a sample and all other points in the same class.\n",
    "- **b**: The mean distance between a sample and all other points in the next nearest cluster.\n",
    "\n",
    "\n",
    "**Adjusted Rand Index (ARI):**\n",
    "\n",
    "$$\n",
    "ARI = \\frac{RI - Expected\\_RI}{max(RI) - Expected\\_RI}\n",
    "$$\n",
    "\n",
    "$$\n",
    "RI = \\frac{a + b}{a + b + c + d}\n",
    "$$\n",
    "\n",
    "\n",
    "- a, the number of pairs of elements in S that are in the same subset in X and in the same subset in Y\n",
    "- b, the number of pairs of elements in S that are in different subsets in X and in different subsets in Y\n",
    "- c, the number of pairs of elements in S that are in the same subset in X and in different subsets in Y\n",
    "- d, the number of pairs of elements in S that are in different subsets in X and in the same subset in Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "silhouette_score_values = []\n",
    "adjusted_rand_score_values = []\n",
    "within_cluster_dist_values = []\n",
    "n_clusters = np.arange(2, 21)\n",
    "\n",
    "for n in n_clusters:\n",
    "    \n",
    "    clusterer = MyKmeans(n_clusters=n, max_iter=10, n_init=10)\n",
    "    clusterer.fit(X)\n",
    "    y_pred = clusterer.predict(X)\n",
    "    \n",
    "    score1 = metrics.silhouette_score(X, y_pred)\n",
    "    silhouette_score_values.append(score1)\n",
    "    \n",
    "    score2 = metrics.adjusted_rand_score(y, y_pred)\n",
    "    adjusted_rand_score_values.append(score2)\n",
    "    \n",
    "    score3 = clusterer.best_metric\n",
    "    within_cluster_dist_values.append(score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(n_clusters, silhouette_score_values, linewidth=3, label='Silhouette score')\n",
    "plt.plot(n_clusters, adjusted_rand_score_values, linewidth=3, label='Adjusted rand score')\n",
    "plt.xlabel('Number of clusters', size=16)\n",
    "plt.ylabel('Score', size=16)\n",
    "plt.xticks(n_clusters, size=16)\n",
    "plt.yticks(size=16)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid(b=1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(n_clusters, within_cluster_dist_values, linewidth=3, label='Within-cluster distance')\n",
    "plt.xlabel('Number of clusters', size=16)\n",
    "plt.ylabel('Score', size=16)\n",
    "plt.xticks(n_clusters, size=16)\n",
    "plt.yticks(size=16)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Clustering Algorithms\n",
    "\n",
    "Short overview of other clustering algorithms you can find in `scikit-learn` library [here](https://scikit-learn.org/stable/modules/clustering.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/clusters.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(centers=3, n_samples=n_samples, random_state=random_state, center_box=(-10, 10))\n",
    "\n",
    "# To play with\n",
    "# X, y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "# X, y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "# X = np.random.rand(n_samples, 2)\n",
    "\n",
    "plot_clusters(X, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clustering algorithms\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiniBatchKMeans\n",
    "\n",
    "# Run clusterer\n",
    "clusterer = cluster.MiniBatchKMeans(n_clusters=3, batch_size=100)\n",
    "clusterer.fit(X)\n",
    "y_pred = clusterer.predict(X)\n",
    "\n",
    "# Plot clustering results\n",
    "plot_clusters(X, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN:**\n",
    "\n",
    "<center><img src=\"img/dbscan.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "\n",
    "# Run clusterer\n",
    "clusterer = cluster.DBSCAN(eps=0.5, min_samples=5)\n",
    "y_pred = clusterer.fit_predict(X)\n",
    "\n",
    "# Plot clustering results\n",
    "plot_clusters(X, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agglomerative Clustering:**\n",
    "\n",
    "<center><img src=\"img/agglo.png\" width=\"600\"></center>\n",
    "\n",
    "[image link](https://quantdare.com/hierarchical-clustering/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgglomerativeClustering\n",
    "\n",
    "# Run clusterer\n",
    "clusterer = cluster.AgglomerativeClustering(n_clusters=3)\n",
    "y_pred = clusterer.fit_predict(X)\n",
    "\n",
    "# Plot clustering results\n",
    "plot_clusters(X, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "\n",
    "- Rerun cells above for other datasets. Explain the clustering results.\n",
    "- Try different number of clusters and other options. How can you explain what you see?\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(centers=3, n_samples=n_samples, random_state=random_state, center_box=(-10, 10))\n",
    "\n",
    "# To play with\n",
    "# X, y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "# X, y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "# X = np.random.rand(n_samples, 2)\n",
    "\n",
    "plot_clusters(X, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply one of the sample features by a large number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X.copy()\n",
    "X_scaled[:, 1] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X_scaled, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering without scaling\n",
    "\n",
    "All clustering algorithms are based on distances between objects $\\rho(x_{i}, x_{j})$. For an axample in 2D case:\n",
    "\n",
    "$$\n",
    "\\rho(x_{i}, x_{j}) = \\sqrt{ (x_{1i} - x_{1j})^{2} + (x_{2i} - x_{2j})^{2} }\n",
    "$$\n",
    "\n",
    "where $x_{1i}$ is the 1st input feature, $x_{2i}$ is the 2nd one.\n",
    "\n",
    "Suppose, that the features have different scales:\n",
    "\n",
    "$$\n",
    "\\frac{x_{2i}}{x_{1i}} = 1000\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\rho(x_{i}, x_{j}) = \\sqrt{ (x_{1i} - x_{1j})^{2} + (x_{2i} - x_{2j})^{2} } \\approx\\sqrt{ (x_{2i} - x_{2j})^{2} }\n",
    "$$\n",
    "\n",
    "So, the 1st feature will not be taken into account by clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "# Run clustering algorithm\n",
    "clusterer = cluster.KMeans(n_clusters=3, n_init=10)\n",
    "clusterer.fit(X_scaled)\n",
    "y_pred = clusterer.predict(X_scaled)\n",
    "\n",
    "# Show clustering results\n",
    "plot_clusters(X_scaled, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with Standard Scaler\n",
    "\n",
    "Satndard Scaler transforms feature $x$ to a new feature $x_{new}$ with zero mean and unit variance by the following way:\n",
    "\n",
    "$$\n",
    "x_{new} = \\frac{ x - \\mu }{ \\sigma }\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{N} \\sum_{i=1}^{N}x_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^{N} (x_{i} - \\mu)^{2} }\n",
    "$$\n",
    "\n",
    "This transforms all input features to the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_scaled)\n",
    "X_scaled_ss = ss.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "# Run clustering algorithm\n",
    "clusterer = cluster.KMeans(n_clusters=3, n_init=10)\n",
    "clusterer.fit(X_scaled_ss)\n",
    "y_pred = clusterer.predict(X_scaled_ss)\n",
    "\n",
    "# Show clustering results\n",
    "plot_clusters(X_scaled, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Generate 2D Gaussian distribution\n",
    "n_samples = 2000\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=42, centers=[[0, 0]])\n",
    "\n",
    "# Apply coordiantes transformation\n",
    "transformation = [[0.6, 0.4], \n",
    "                  [0.4, 0.6]]\n",
    "X_aniso = np.dot(X, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X_aniso, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Find directions along which our datapoints have the greatest variance:\n",
    "\n",
    "\n",
    "<center><img src='http://www.visiondummy.com/wp-content/uploads/2014/05/correlated_2d.png' width=300></center>\n",
    "\n",
    "\n",
    "These directions are principal components. Principal components $a_{1},a_{2},...a_{D}\\in\\mathbb{R}^{D}$ are orthonormal: \n",
    "\n",
    "$$\n",
    "\\langle a_{i},a_{j}\\rangle=\\begin{cases}\n",
    "1, & i=j\\\\\n",
    "0 & i\\ne j\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA algorithm (detailed):\n",
    "\n",
    "### Step 1:\n",
    "Calculate variance across a principal component $a$ assuming that $X$ is centralized:\n",
    "$$\n",
    "\\begin{align} \\sigma^2_a & = \\frac{1}{n}\\sum\\limits_{i=1}^n(a^\\top x_i - \\mu)^2 \\\\\n",
    "& = \\frac{1}{n}\\sum\\limits_{i=1}^n(a^\\top x_i - 0)^2 \\\\\n",
    "& = \\frac{1}{n}\\sum\\limits_{i=1}^n a^\\top( x_i x_i^\\top) a \\\\\n",
    "& = a^\\top \\left(\\frac{1}{n}\\sum\\limits_{i=1}^n x_i x_i^\\top \\right) a \\\\\n",
    "& = a^\\top X^\\top X a \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "Find $a_1$ that maximizes the variance:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "a_1^\\top X^\\top X a_1 \\rightarrow \\max_{a_1} \\\\\n",
    "a_1^\\top a_1 = 1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Lagrangian of optimization problem:\n",
    "$$ \\mathcal{L}(a_1, \\nu) = a_1^\\top X^\\top X a_1 - \\nu (a_1^\\top a_1 - 1) \\rightarrow max_{a_1, \\nu}$$\n",
    "\n",
    "Derivative w.r.t. $a_1$:\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial a_1} = 2X^\\top X a_1 - 2\\nu a_1 = 0 $$\n",
    "\n",
    "$$X^\\top X a_1 = \\nu a_1$$\n",
    "\n",
    "---\n",
    "#### Note:\n",
    "So $a_1$ is selected from a set of eigenvectors of  $X^\\top X$. But which one?\n",
    "\n",
    "$$ a_1^\\top X^\\top X a_1 = \\nu a_1^\\top a_1 = \\nu \\rightarrow \\max$$\n",
    "\n",
    "That means:\n",
    "* $\\nu$ should be the greatest eigenvalue of matrix $X^\\top X$, which is $\\lambda_1$\n",
    "* $a_1$ is eigenvector, correspondent to $\\lambda_1$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:\n",
    "\n",
    "Similarly for $a_{2}$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "a_2^\\top X^\\top X a_2 \\rightarrow \\max_{a_2} \\\\\n",
    "a_2^\\top a_2 = 1 \\\\\n",
    "a_2^\\top a_1 = 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA algorithm (short)\n",
    "\n",
    "1. Center (and scale) dataset\n",
    "2. Calculate covariance matrix $С=X^\\top X$\n",
    "3. Find first $k$ eigenvalues and eigenvectors\n",
    "$$A = \n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\mid & \\mid & & \\mid\\\\\n",
    "    a_{1} & a_{2} & \\ldots & a_{k} \\\\\n",
    "    \\mid & \\mid & & \\mid \n",
    "  \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "4. Perform projection:\n",
    "$$ Z = XA $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_aniso)\n",
    "\n",
    "# Apply PCA\n",
    "X_aniso_pca = pca.transform(X_aniso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aniso_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1 = PCA(n_components=1)\n",
    "pca_1.fit(X_aniso)\n",
    "X_aniso_pca_1 = pca_1.transform(X_aniso)\n",
    "\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2.fit(X_aniso)\n",
    "X_aniso_pca_2 = pca_2.transform(X_aniso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot original X with eigenvectors\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], color='b')\n",
    "for vector in pca_2.components_:\n",
    "    plt.arrow(0, 0, vector[0], vector[1], head_width=0.2, head_length=0.2, fc='k', ec='k')\n",
    "plt.title(\"Original X\", size=14)\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "#plt.grid(b=1)\n",
    "plt.legend(loc='best')\n",
    "\n",
    "\n",
    "# Plot for PCA with n_components=2\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_aniso_pca_2[:, 0], X_aniso_pca_2[:, 1], color='b')\n",
    "for vector in pca_2.transform(pca_2.components_):\n",
    "    plt.arrow(0, 0, vector[0], vector[1], head_width=0.2, head_length=0.2, fc='k', ec='k', linewidth=2)\n",
    "plt.title(\"PCA with n_component = 2\", size=14)\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "#plt.grid(b=1)\n",
    "\n",
    "\n",
    "# Plot for PCA with n_components=1\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_aniso_pca_1[:, 0], [0]*len(X_aniso_pca_1), color='b')\n",
    "for vector in pca_1.transform(pca_1.components_):\n",
    "    if vector[0] <= 0.5: continue\n",
    "    plt.arrow(0, 0, vector[0], 0, head_width=0.2, head_length=0.2, fc='k', ec='k', linewidth=2)\n",
    "plt.title(\"PCA with n_component = 1\", size=14)\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.xlim(-4.5, 4.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "#plt.grid(b=1)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Recognition by Voice\n",
    "\n",
    "This database was created to identify a voice as male or female, based upon acoustic properties of the voice and speech. The dataset consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages, with an analyzed frequency range of 0hz-280hz (human vocal range).\n",
    "\n",
    "The following acoustic properties of each voice are measured and included within the CSV:\n",
    "\n",
    "* meanfreq: mean frequency (in kHz)\n",
    "* sd: standard deviation of frequency\n",
    "* median: median frequency (in kHz)\n",
    "* Q25: first quantile (in kHz)\n",
    "* Q75: third quantile (in kHz)\n",
    "* IQR: interquantile range (in kHz)\n",
    "* skew: skewness (see note in specprop description)\n",
    "* kurt: kurtosis (see note in specprop description)\n",
    "* sp.ent: spectral entropy\n",
    "* sfm: spectral flatness\n",
    "* mode: mode frequency\n",
    "* centroid: frequency centroid (see specprop)\n",
    "* peakf: peak frequency (frequency with highest energy)\n",
    "* meanfun: average of fundamental frequency measured across acoustic signal\n",
    "* minfun: minimum fundamental frequency measured across acoustic signal\n",
    "* maxfun: maximum fundamental frequency measured across acoustic signal\n",
    "* meandom: average of dominant frequency measured across acoustic signal\n",
    "* mindom: minimum of dominant frequency measured across acoustic signal\n",
    "* maxdom: maximum of dominant frequency measured across acoustic signal\n",
    "* dfrange: range of dominant frequency measured across acoustic signal\n",
    "* modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n",
    "* label: male or female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/yandexdataschool/mlhep2019/master/notebooks/day-2/Clustering/data/voice.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data sample\n",
    "data = pd.read_csv(\"voice.csv\")\n",
    "print(\"DataFrame shape: \", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = data.columns.drop(['label'])\n",
    "print(\"Feature names: \", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = data[feature_names].values\n",
    "y = 1. * (data['label'].values == 'male')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split + Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data into train and test samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardization\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# You can play with other classifiers\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = clf.predict(X_test)\n",
    "y_test_proba = clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_predict)\n",
    "auc = roc_auc_score(y_test, y_test_proba)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"ROC AUC: \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_accuracies = []\n",
    "pca_aucs = []\n",
    "pca_components = np.arange(1, 21)\n",
    "\n",
    "for n_components in pca_components:\n",
    "    \n",
    "    # For each n_components run PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_train)\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Fit a classifier\n",
    "    clf = LogisticRegression(solver='lbfgs')\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_test_predict = clf.predict(X_test_pca)\n",
    "    y_test_proba = clf.predict_proba(X_test_pca)[:, 1]\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    accuracy = accuracy_score(y_test, y_test_predict)\n",
    "    pca_accuracies.append(accuracy)\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_test_proba)\n",
    "    pca_aucs.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(pca_components, pca_accuracies, label='Accuracy', color='b', linewidth=3)\n",
    "plt.plot(pca_components, pca_aucs, label='ROC AUC', color='r', linewidth=3)\n",
    "plt.xticks(pca_components, size=14)\n",
    "plt.xlabel(\"N components of PCA\", size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.ylabel(\"Metric values\", size=14)\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained variance for $a_i$ can be calculated as the following ratio:\n",
    "$$\n",
    "\\frac{\\lambda_{i}}{\\sum_{d=1}^{D}\\lambda_{d}}\n",
    "$$\n",
    "\n",
    "where $\\lambda_{i}$ is an eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Take all eigenvalues (sorted)\n",
    "eigenvalues = pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = np.arange(1, 21)\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance = eigenvalues / eigenvalues.sum()\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumsum_explained_variance = np.cumsum(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(pca_components, explained_variance, color='b', linewidth=3)\n",
    "plt.xticks(pca_components, size=14)\n",
    "plt.xlabel(\"N components of PCA\", size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.ylabel(\"Explained Variance\", size=14)\n",
    "plt.title(\"PCA Explained Variance\", size=14)\n",
    "plt.grid(b=1)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(pca_components, cumsum_explained_variance, color='b', linewidth=3)\n",
    "plt.xticks(pca_components, size=14)\n",
    "plt.xlabel(\"N components of PCA\", size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.ylabel(\"Explained Variance\", size=14)\n",
    "plt.title(\"PCA Cumulative Explained Variance\", size=14)\n",
    "plt.grid(b=1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Anomalies Detection\n",
    "\n",
    "\n",
    "`Scikit-learn` has several anomalies detection algorithms. Their description and examples are provided on [this page](https://scikit-learn.org/stable/modules/outlier_detection.html):\n",
    "\n",
    "<center><img src=\"img/anomaly.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Define key constants\n",
    "n_samples = 500\n",
    "outliers_fraction = 0.15\n",
    "n_outliers = int(outliers_fraction * n_samples)\n",
    "\n",
    "# Generate sample\n",
    "X = 4 * (datasets.make_moons(n_samples=n_samples, noise=.05, random_state=0)[0] - np.array([0.5, 0.25]))\n",
    "\n",
    "# Add outliers\n",
    "X = np.concatenate([X, np.random.RandomState(42).uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies(X, ano):\n",
    "    \n",
    "    if ano is not None:\n",
    "        try:\n",
    "            y = ano.predict(X)\n",
    "        except:\n",
    "            y = ano.fit_predict(X)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    # Create an figure with a custom size\n",
    "    # plt.figure(figsize=(9, 6))\n",
    "    \n",
    "    if y is not None:\n",
    "        for cluster_label in np.unique(y):\n",
    "            \n",
    "            if cluster_label == -1:\n",
    "                suff = \" (Anomaly)\"\n",
    "            else:\n",
    "                suff = \" (Normal)\"\n",
    "            \n",
    "            # Plot all objects with y == i (class 0)\n",
    "            plt.scatter(X[y == cluster_label, 0],          # selects all objects with y == i and the 1st column of X\n",
    "                        X[y == cluster_label, 1],          # selects all objects with y == i and the 2nd column of X\n",
    "                        label=str(cluster_label)+suff)     # label for the plot legend\n",
    "    else:\n",
    "        plt.scatter(X[:, 0], X[:, 1], label='samples')\n",
    "        \n",
    "    \n",
    "    # Plot decision doundary\n",
    "    if ano is not None:\n",
    "        xx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\n",
    "        try:\n",
    "            Z = ano.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.xlabel('X1', size=12) # set up X-axis label\n",
    "    plt.ylabel('X2', size=12) # set up Y-axis label\n",
    "    plt.xticks(size=12)\n",
    "    plt.yticks(size=12)\n",
    "\n",
    "    plt.legend(loc='best', fontsize=12) # create the plot legend and set up it position\n",
    "    plt.grid(b=1) # create grid on the plot\n",
    "\n",
    "    plt.show() # display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(X, ano=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Anomalies Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IsolationForest\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Run anomalies detection algorithm\n",
    "ano = IsolationForest(behaviour='new', contamination=outliers_fraction, random_state=42)\n",
    "ano.fit(X)\n",
    "\n",
    "# Detect anomalies\n",
    "y_pred = ano.predict(X)\n",
    "\n",
    "# Plot detected anomalies\n",
    "plot_anomalies(X, ano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EllipticEnvelope (Robust covariance)\n",
    "\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "ano = EllipticEnvelope(contamination=outliers_fraction)\n",
    "ano.fit(X)\n",
    "\n",
    "# Detect anomalies\n",
    "y_pred = ano.predict(X)\n",
    "\n",
    "# Plot detected anomalies\n",
    "plot_anomalies(X, ano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "\n",
    "# Run anomalies detection algorithm\n",
    "ano = cluster.DBSCAN(eps=0.5, min_samples=5)\n",
    "ano.fit(X)\n",
    "\n",
    "# Detect anomalies\n",
    "y_pred = ano.fit_predict(X)\n",
    "\n",
    "# Plot detected anomalies\n",
    "plot_anomalies(X, ano)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "* Change `contamination` parameter. How can you explain the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
