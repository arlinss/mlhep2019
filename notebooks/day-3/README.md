Lecture slides: __[tinyurl.com/mlhep19-dl-intro](https://tinyurl.com/mlhep19-dl-intro)__
Lecture slides part2: __[tinyurl.com/mlhep19-dl-convnets](https://tinyurl.com/mlhep19-dl-convnets)__
Lecture slides part3: __[yadi.sk/...](https://yadi.sk/i/eyCRxQHx3NH2dg)___

Practice pytorch: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/mlhep2019/blob/master/notebooks/day-3/seminar_pytorch.ipynb)

Practice convnets: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/mlhep2019/blob/master/notebooks/day-3/seminar_convnets.ipynb)

Practice model zoo: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/Practical_DL/blob/spring2019/week04_finetuning/seminar_pytorch.ipynb)

Running locally: as usual, go to `seminar_pytorch.ipynb` and follow instructions from there.


## Materials
* A lecture on backprop (karpathy) - [video](https://www.youtube.com/watch?v=59Hbtz7XgjM)
  * [Backprop by cs231](http://cs231n.github.io/optimization-2/)
  * [alternative] A more classical lecture on neural networks (english) - [video](https://www.youtube.com/watch?v=uXt8qF2Zzfo)
* Stochastic gradient descent modiffications - [video](https://www.youtube.com/watch?v=nhqo0u1a6fw)
  * A blog post overview of gradient descent methods - [url](http://ruder.io/optimizing-gradient-descent/)
* Deep learning frameworks (english) - [video](https://www.youtube.com/watch?v=Vf_-OkqbwPo)

## Convolutional neural networks
* Convolutional networks (by A. Karpathy) - [video](https://www.youtube.com/watch?v=AQirPKrAyDg)
  * http://cs231n.github.io/convolutional-networks/
  * http://cs231n.github.io/understanding-cnn/
  * [a deep learning neophite cheat sheet](http://www.kdnuggets.com/2016/03/must-know-tips-deep-learning-part-1.html)
  * [more stuff for vision](https://bavm2013.splashthat.com/img/events/46439/assets/34a7.ranzato.pdf)
  * a [CNN trainer in a browser](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)
  * Bonus: [graph convolutions](https://colab.research.google.com/drive/155nh8rZ63C7EWBNhbSJzYdab92hPHMTH)



## More on adaptive optimization
* Interactive [neural network playground](http://playground.tensorflow.org/) in your browser
* [Cool interactive demo of momentum](http://distill.pub/2017/momentum/)
* A wonderful [blog post](http://karpathy.github.io/2019/04/25/recipe/) on what to expect from deep learning
* [Notation](http://cs231n.github.io/neural-networks-1/#nn)
* [wikipedia on SGD :)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), expecially the "extensions and variants" section


## More on DL frameworks
  - A lecture on nonlinearities, intializations and other tricks in deep learning (karpathy) - [video](https://www.youtube.com/watch?v=GUtlrDbHhJM)
  - A lecture on activations, recap of adaptive SGD and dropout (karpathy) - [video](https://www.youtube.com/watch?v=KaR4lIdI1MQ)
  - [a deep learning neophite cheat sheet](http://www.kdnuggets.com/2016/03/must-know-tips-deep-learning-part-1.html)
  - [bonus video] Deep learning philosophy: [our humble take](https://www.youtube.com/watch?v=9qyE1Ev1Xdw) (english)
  - [reading] on weight initialization: [blog post](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)
  - [reading] pretty much all the module 1 of http://cs231n.github.io/

