{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7JGD6RlLYVz"
   },
   "source": [
    "# Learning to pivot\n",
    "\n",
    "Main paper: https://arxiv.org/abs/1611.01046\n",
    "\n",
    "In the notebook, we are going to make classifier's predictions independent from a *nuisance* parameters.\n",
    "While nuisance parameters themselves can be not explicitely present in the dataset, they can be partially inffered from the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import mlhep2019\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess as sp\n",
    "    result = sp.run(\n",
    "        ['pip', 'install', 'git+https://github.com/yandexdataschool/mlhep2019.git'],\n",
    "        stdout=sp.PIPE, stderr=sp.PIPE\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(result.stdout.decode('utf-8'))\n",
    "        print(result.stderr.decode('utf-8'))\n",
    "    \n",
    "    import mlhep2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q56Jl-vVBqqy"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from mlhep2019.pivot import get_susy, split\n",
    "from mlhep2019.pivot import nuisance_metric_plot, nuisance_prediction_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OF5sS9uREo_x",
    "outputId": "af277d3a-2f9d-498d-f58d-3458e4b07f93"
   },
   "outputs": [],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "  print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQ24YaviBqrB"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "  print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTVi7FB0BqrF"
   },
   "source": [
    "## Downloading SUSY dataset\n",
    "\n",
    "The dataset can be found at https://archive.ics.uci.edu/ml/datasets/SUSY\n",
    "\n",
    "The original paper is:\n",
    "Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014)\n",
    "\n",
    "### Data Set Information:\n",
    "\n",
    "Provide all relevant informatioThe data has been produced using Monte Carlo simulations. The first 8 features are kinematic properties measured by the particle detectors in the accelerator. The last ten features are functions of the first 8 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks and the dropout algorithm are presented in the original paper. The last 500,000 examples are used as a test set.n about your data set.\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "The first column is the class label (1 for signal, 0 for background), followed by the 18 features (8 low-level features then 10 high-level features):: lepton 1 pT, lepton 1 eta, lepton 1 phi, lepton 2 pT, lepton 2 eta, lepton 2 phi, missing energy magnitude, missing energy phi, MET_rel, axial MET, M_R, M_TR_2, R, MT2, S_R, M_Delta_R, dPhi_r_b, cos(theta_r1). For detailed information about each feature see the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSH6_vG1BqrH"
   },
   "outputs": [],
   "source": [
    "data, labels = get_susy('SUSY/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, labels_train, data_test, labels_test = split(data, labels, split_ratios=(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean, data_std = np.mean(data_train, axis=0), np.std(data_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train -= data_mean[None, :]\n",
    "data_train /= data_std[None, :]\n",
    "\n",
    "### never use test statistics to transform the test dataset!\n",
    "data_test -= data_mean[None, :]\n",
    "data_test /= data_std[None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select the first feature 'lepton 1 pT' as nuisance parameter. Feel free to try a different nusiance parameter, or, perhaps, several nuisance parameters at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, nuisance_train = data_train[:, 1:], data_train[:, 0]\n",
    "data_test, nuisance_test = data_test[:, 1:], data_test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    [nuisance_train[labels_train > 0.5], nuisance_train[labels_train < 0.5]],\n",
    "    bins=100, histtype='step', label=['class 0', 'class 1'], log=True\n",
    ")\n",
    "plt.xlabel('nuisance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, z_train, z_test = [\n",
    "    torch.tensor(x).to(device)\n",
    "    for x in [data_train, data_test, labels_train, labels_test, nuisance_train, nuisance_test]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = torch.utils.data.TensorDataset(X_test, y_test, z_test)\n",
    "loader_test =  torch.utils.data.DataLoader(dataset_test, batch_size=1024, shuffle=False)\n",
    "\n",
    "def test_predictions(model):\n",
    "    with torch.no_grad():\n",
    "        return np.concatenate([\n",
    "            torch.sigmoid(model(X_batch)).to('cpu').detach().numpy()\n",
    "            for X_batch, _, _ in loader_test\n",
    "        ], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "- choose between conditional and unconditional pivoting (or both);\n",
    "- implement and train ordinary classifier;\n",
    "- implement and train pivoted classifier;\n",
    "- compare ROC AUC score depending on nuisance parameter;\n",
    "- make sure, that the Mutual Information (MI) for pivoted classifier is lower than MI for ordinary classifier.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "SUSY and HIGGS datasets are quite difficult classification problems.\n",
    "It is recommended to use networks with 3-5 dense layers, 100+ units each.\n",
    "\n",
    "### Adversary\n",
    "\n",
    "- The nuisance parameter is continuous, therefore, a regression loss should be used (e.g. MSE, MAE).\n",
    "- Adversary should be a small network, 1 hidden layer with ~64 units is fine.\n",
    "- Try to find a good coefficient $\\lambda$ in $\\mathcal{L}_\\mathrm{clf} - \\lambda \\mathcal{L}_\\mathrm{adv}$.\n",
    "\n",
    "\n",
    "### Extra tasks\n",
    "(can be implemented out of order)\n",
    "\n",
    "- try to make adversary loss extremely high (more than $10$ times higher than the loss of the optimal constant), but:\n",
    "    - use 16 steps of the adversary training per one step of the classifier training;\n",
    "    - adversary must converge on its own with fixed classifier;\n",
    "- implement both types of pivoting;\n",
    "- include nuisance parameter in the feature set (keeping it as nuisance parameter), and train pivoted classifier;\n",
    "- try several nuisance parameters at once;\n",
    "- compare different regression losses for the adversary (e.g. MSE vs MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    ### define classifier here\n",
    "    ### don't forget about torch.squeeze\n",
    "    def __init__(self, ):\n",
    "        super(Classifier, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier().to(device)\n",
    "\n",
    "### define loss here\n",
    "loss_fn_clf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output = clf(torch.randn(3, data_train.shape[0]).to(device))\n",
    "example_output_shape = example_output.to('cpu').detach().numpy().shape\n",
    "example_labels = torch.randint(0, 2, size=(3, )).float().to(device)\n",
    "\n",
    "example_loss = loss_fn_clf(example_output, example_labels).to('cpu').detach().numpy()\n",
    "\n",
    "assert example_output_shape == (3, ) or shexample_output_shapeape == (3, 2), \\\n",
    "    'Output shape must be either (3, ) or (3, 2)'\n",
    "\n",
    "assert example_loss.shape == tuple(), 'Check loss implementation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoches = 64\n",
    "n_batches = 4096\n",
    "\n",
    "losses = np.zeros((n_epoches, n_batches), dtype='float32')\n",
    "\n",
    "opt_clf = torch.optim.Adam(clf.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(n_epoches):\n",
    "    for j in range(n_batches):\n",
    "        ### define training procedure here\n",
    "        indx = torch.randint(0, data_train.shape[0], size=(32, ))\n",
    "        X_batch, y_batch, z_batch = X_train[indx], y_train[indx], z_train[indx]\n",
    "        \n",
    "        losses[i, j] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(classifier=losses_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.mean(losses_clf[-1, :]) < np.log(2), 'Classifier seems to not learn anything'\n",
    "assert np.mean(losses_clf[-1, :]) > 0, 'Perhaps, you forgot to fill `losses_clf` array with actual loss values?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adversary(torch.nn.Module):\n",
    "    ### define adversary here\n",
    "    ### don't forget about torch.squeeze\n",
    "    def __init__(self, ):\n",
    "        super(Adversary, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclf = Classifier().to(device)\n",
    "\n",
    "### define pivoted classifier loss here\n",
    "loss_fn_pclf = None\n",
    "\n",
    "adv = Adversary().to(device)\n",
    "\n",
    "### define adversary loss here\n",
    "loss_fn_adv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert clf(torch.randn(3, data_train.shape[0]).to(device)).to('cpu').detach().numpy().shape == (3, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoches = 64\n",
    "n_batches = 1024\n",
    "\n",
    "losses_clf = np.zeros((n_epoches, n_batches), dtype='float32')\n",
    "losses_adv = np.zeros((n_epoches, n_batches), dtype='float32')\n",
    "\n",
    "opt_pclf = torch.optim.Adam(pclf.parameters(), lr=1e-3)\n",
    "opt_adv = torch.optim.Adamax(adv.parameters(), lr=5e-3)\n",
    "\n",
    "for i in range(n_epoches):\n",
    "    for j in range(n_batches):\n",
    "        for k in range(16):\n",
    "            indx = torch.randint(0, data_train.shape[0], size=(32, ))\n",
    "            X_batch, y_batch, z_batch = X_train[indx], y_train[indx], z_train[indx]\n",
    "            \n",
    "            ### define adversary training here\n",
    "    \n",
    "        indx = torch.randint(0, data_train.shape[0], size=(32, ))\n",
    "        X_batch, y_batch, z_batch = X_train[indx], y_train[indx], z_train[indx]\n",
    "        \n",
    "        ### define pivoted classifier training here\n",
    "        \n",
    "        losses_clf[i, j] = 0.0\n",
    "        losses_adv[i, j] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(classifier=losses_clf, adversary=losses_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.mean(losses_clf[-1, :]) < np.log(2), 'The classifier seems to not learn anything.'\n",
    "assert np.mean(losses_clf[-1, :]) > 0, 'Perhaps, you forgot to fill `losses_clf` array with actual loss values?'\n",
    "assert np.mean(losses_adv[-1, :]) > 0, 'Perhaps, you forgot to fill `losses_adv` array with actual loss values?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuisance_prediction_hist([\n",
    "        test_predictions(clf),\n",
    "        test_predictions(pclf),\n",
    "    ],\n",
    "    nuisance_test, nuisance_bins=6,\n",
    "    labels=labels_test.astype('int'),\n",
    "    names=['non-pivoted', 'pivoted']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows dependency between predictions and the nuisance parameter:\n",
    "- each column correspond to a different model;\n",
    "- rows correspond to nuisance parameter bins;\n",
    "- each plot show distribution of model predictions within the corresponding nuisance bin.\n",
    "\n",
    "- $\\mathrm{MI}$ - (unconditional) mutual information between the nuisance parameter and model predictions.\n",
    "- $\\mathrm{MI}_i$ - mutual information between the nuisance parameter and model predictions **within** $i$-th class.\n",
    "\n",
    "**Note**, that the following Mutual Information estimates migh be unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuisance_metric_plot([\n",
    "        test_predictions(clf),\n",
    "        test_predictions(pclf),\n",
    "    ],\n",
    "    labels=labels_test,\n",
    "    nuisance=nuisance_test.astype('int'),\n",
    "    metric_fn=roc_auc_score, metric_name='ROC AUC',\n",
    "    names=['non-pivoted', 'pivoted']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pivot.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
